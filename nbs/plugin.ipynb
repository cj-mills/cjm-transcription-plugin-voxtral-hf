{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "37880f73",
   "metadata": {},
   "source": [
    "# Voxtral HF Plugin\n",
    "\n",
    "> Plugin implementation for Mistral Voxtral transcription through Hugging Face Transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e9838a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp plugin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96837562",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "from nbdev.showdoc import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2fbcf2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "import json\n",
    "import logging\n",
    "from pathlib import Path\n",
    "from typing import Dict, Any, Optional, List, Union, Generator\n",
    "import tempfile\n",
    "import warnings\n",
    "from threading import Thread\n",
    "\n",
    "from fastcore.basics import patch\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import soundfile as sf\n",
    "\n",
    "try:\n",
    "    from transformers import VoxtralForConditionalGeneration, AutoProcessor\n",
    "    from transformers import TextStreamer, TextIteratorStreamer\n",
    "    VOXTRAL_AVAILABLE = True\n",
    "except ImportError:\n",
    "    VOXTRAL_AVAILABLE = False\n",
    "    \n",
    "from cjm_transcription_plugin_system.plugin_interface import PluginInterface\n",
    "from cjm_transcription_plugin_system.core import AudioData, TranscriptionResult"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28e027a7-577a-41ac-8f13-bf84a68b5adb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class VoxtralHFPlugin(PluginInterface):\n",
    "    \"\"\"Mistral Voxtral transcription plugin via Hugging Face Transformers.\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        \"\"\"Initialize the Voxtral HF plugin with default configuration.\"\"\"\n",
    "        self.logger = logging.getLogger(f\"{__name__}.{type(self).__name__}\")\n",
    "        self.config = {}\n",
    "        self.model = None\n",
    "        self.processor = None\n",
    "        self.device = None\n",
    "        self.dtype = None\n",
    "    \n",
    "    @property\n",
    "    def name(\n",
    "        self\n",
    "    ) -> str:  # Returns the plugin name\n",
    "        \"\"\"Get the plugin name identifier.\"\"\"\n",
    "        return \"voxtral_hf\"\n",
    "    \n",
    "    @property\n",
    "    def version(\n",
    "        self\n",
    "    ) -> str:  # Returns the plugin version\n",
    "        \"\"\"Get the plugin version string.\"\"\"\n",
    "        return \"1.0.0\"\n",
    "    \n",
    "    @property\n",
    "    def supported_formats(\n",
    "        self\n",
    "    ) -> List[str]:  # Returns list of supported audio formats\n",
    "        \"\"\"Get the list of supported audio file formats.\"\"\"\n",
    "        return [\"wav\", \"mp3\", \"flac\", \"m4a\", \"ogg\", \"webm\", \"mp4\", \"avi\", \"mov\"]\n",
    "    \n",
    "    def get_config_schema(\n",
    "        self\n",
    "    ) -> Dict[str, Any]:  # Returns the configuration schema dictionary\n",
    "        \"\"\"Return configuration schema for Voxtral HF.\"\"\"\n",
    "        return {\n",
    "            \"$schema\": \"http://json-schema.org/draft-07/schema#\",\n",
    "            \"type\": \"object\",\n",
    "            \"title\": \"Voxtral HF Configuration\",\n",
    "            \"properties\": {\n",
    "                \"model_id\": {\n",
    "                    \"type\": \"string\",\n",
    "                    \"enum\": [\"mistralai/Voxtral-Mini-3B-2507\", \"mistralai/Voxtral-Small-24B-2507\"],\n",
    "                    \"default\": \"mistralai/Voxtral-Mini-3B-2507\",\n",
    "                    \"description\": \"Voxtral model to use. Mini is faster, Small is more accurate.\"\n",
    "                },\n",
    "                \"device\": {\n",
    "                    \"type\": \"string\",\n",
    "                    \"enum\": [\"auto\", \"cpu\", \"cuda\"],\n",
    "                    \"default\": \"auto\",\n",
    "                    \"description\": \"Device for inference (auto will use CUDA if available)\"\n",
    "                },\n",
    "                \"dtype\": {\n",
    "                    \"type\": \"string\",\n",
    "                    \"enum\": [\"auto\", \"bfloat16\", \"float16\", \"float32\"],\n",
    "                    \"default\": \"auto\",\n",
    "                    \"description\": \"Data type for model weights (auto will use bfloat16 on GPU, float32 on CPU)\"\n",
    "                },\n",
    "                \"language\": {\n",
    "                    \"type\": [\"string\", \"null\"],\n",
    "                    \"default\": \"en\",\n",
    "                    \"description\": \"Language code for transcription (e.g., 'en', 'es', 'fr')\",\n",
    "                    \"examples\": [\"en\", \"es\", \"fr\", \"de\", \"it\", \"pt\", \"nl\", \"pl\", \"ru\", \"zh\", \"ja\", \"ko\"]\n",
    "                },\n",
    "                \"max_new_tokens\": {\n",
    "                    \"type\": \"integer\",\n",
    "                    \"minimum\": 1,\n",
    "                    \"maximum\": 50000,\n",
    "                    \"default\": 25000,\n",
    "                    \"description\": \"Maximum number of tokens to generate\"\n",
    "                },\n",
    "                \"do_sample\": {\n",
    "                    \"type\": \"boolean\",\n",
    "                    \"default\": False,\n",
    "                    \"description\": \"Whether to use sampling (true) or greedy decoding (False)\"\n",
    "                },\n",
    "                \"temperature\": {\n",
    "                    \"type\": \"number\",\n",
    "                    \"minimum\": 0.0,\n",
    "                    \"maximum\": 2.0,\n",
    "                    \"default\": 1.0,\n",
    "                    \"description\": \"Temperature for sampling (only used when do_sample=true)\"\n",
    "                },\n",
    "                \"top_p\": {\n",
    "                    \"type\": \"number\",\n",
    "                    \"minimum\": 0.0,\n",
    "                    \"maximum\": 1.0,\n",
    "                    \"default\": 0.95,\n",
    "                    \"description\": \"Top-p (nucleus) sampling parameter (only used when do_sample=true)\"\n",
    "                },\n",
    "                \"streaming\": {\n",
    "                    \"type\": \"boolean\",\n",
    "                    \"default\": False,\n",
    "                    \"description\": \"Enable streaming output (yields partial results as they're generated)\"\n",
    "                },\n",
    "                \"trust_remote_code\": {\n",
    "                    \"type\": \"boolean\",\n",
    "                    \"default\": False,\n",
    "                    \"description\": \"Whether to trust remote code when loading the model\"\n",
    "                },\n",
    "                \"cache_dir\": {\n",
    "                    \"type\": [\"string\", \"null\"],\n",
    "                    \"default\": None,\n",
    "                    \"description\": \"Directory to cache downloaded models\"\n",
    "                },\n",
    "                \"compile_model\": {\n",
    "                    \"type\": \"boolean\",\n",
    "                    \"default\": False,\n",
    "                    \"description\": \"Use torch.compile for potential speedup (requires PyTorch 2.0+)\"\n",
    "                },\n",
    "                \"load_in_8bit\": {\n",
    "                    \"type\": \"boolean\",\n",
    "                    \"default\": False,\n",
    "                    \"description\": \"Load model in 8-bit quantization (requires bitsandbytes)\"\n",
    "                },\n",
    "                \"load_in_4bit\": {\n",
    "                    \"type\": \"boolean\",\n",
    "                    \"default\": False,\n",
    "                    \"description\": \"Load model in 4-bit quantization (requires bitsandbytes)\"\n",
    "                }\n",
    "            },\n",
    "            \"required\": [\"model_id\"],\n",
    "            \"additionalProperties\": False\n",
    "        }\n",
    "    \n",
    "    def get_current_config(\n",
    "        self\n",
    "    ) -> Dict[str, Any]:  # Returns the current configuration dictionary\n",
    "        \"\"\"Return current configuration.\"\"\"\n",
    "        defaults = self.get_config_defaults()\n",
    "        return {**defaults, **self.config}\n",
    "    \n",
    "    def initialize(\n",
    "        self,\n",
    "        config: Optional[Dict[str, Any]] = None  # Configuration dictionary to initialize the plugin\n",
    "    ) -> None:\n",
    "        \"\"\"Initialize the plugin with configuration.\"\"\"\n",
    "        if config:\n",
    "            is_valid, error = self.validate_config(config)\n",
    "            if not is_valid:\n",
    "                raise ValueError(f\"Invalid configuration: {error}\")\n",
    "        \n",
    "        # Merge with defaults\n",
    "        defaults = self.get_config_defaults()\n",
    "        self.config = {**defaults, **(config or {})}\n",
    "        \n",
    "        # Set device\n",
    "        if self.config[\"device\"] == \"auto\":\n",
    "            self.device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "        else:\n",
    "            self.device = self.config[\"device\"]\n",
    "        \n",
    "        # Set dtype\n",
    "        if self.config[\"dtype\"] == \"auto\":\n",
    "            if self.device == \"cuda\":\n",
    "                self.dtype = torch.bfloat16\n",
    "            else:\n",
    "                self.dtype = torch.float32\n",
    "        else:\n",
    "            dtype_map = {\n",
    "                \"bfloat16\": torch.bfloat16,\n",
    "                \"float16\": torch.float16,\n",
    "                \"float32\": torch.float32\n",
    "            }\n",
    "            self.dtype = dtype_map[self.config[\"dtype\"]]\n",
    "        \n",
    "        self.logger.info(f\"Initialized Voxtral HF plugin with model '{self.config['model_id']}' on device '{self.device}' with dtype '{self.dtype}'\")\n",
    "    \n",
    "    def _load_model(\n",
    "        self\n",
    "    ) -> None:\n",
    "        \"\"\"Load the Voxtral model and processor (lazy loading).\"\"\"\n",
    "        if self.model is None or self.processor is None:\n",
    "            try:\n",
    "                self.logger.info(f\"Loading Voxtral model: {self.config['model_id']}\")\n",
    "                \n",
    "                # Load processor\n",
    "                self.processor = AutoProcessor.from_pretrained(\n",
    "                    self.config[\"model_id\"],\n",
    "                    cache_dir=self.config.get(\"cache_dir\"),\n",
    "                    trust_remote_code=self.config.get(\"trust_remote_code\", False)\n",
    "                )\n",
    "                \n",
    "                # Model loading kwargs\n",
    "                model_kwargs = {\n",
    "                    \"cache_dir\": self.config.get(\"cache_dir\"),\n",
    "                    \"trust_remote_code\": self.config.get(\"trust_remote_code\", False),\n",
    "                    \"device_map\": self.device,\n",
    "                }\n",
    "                \n",
    "                # Add quantization settings if specified\n",
    "                if self.config.get(\"load_in_8bit\", False):\n",
    "                    model_kwargs[\"load_in_8bit\"] = True\n",
    "                elif self.config.get(\"load_in_4bit\", False):\n",
    "                    model_kwargs[\"load_in_4bit\"] = True\n",
    "                else:\n",
    "                    model_kwargs[\"dtype\"] = self.dtype\n",
    "                \n",
    "                # Load model\n",
    "                self.model = VoxtralForConditionalGeneration.from_pretrained(\n",
    "                    self.config[\"model_id\"],\n",
    "                    **model_kwargs\n",
    "                )\n",
    "                \n",
    "                # Optionally compile the model (PyTorch 2.0+)\n",
    "                if self.config.get(\"compile_model\", False) and hasattr(torch, 'compile'):\n",
    "                    self.model = torch.compile(self.model)\n",
    "                    self.logger.info(\"Model compiled with torch.compile\")\n",
    "                    \n",
    "                self.logger.info(\"Voxtral model loaded successfully\")\n",
    "            except Exception as e:\n",
    "                raise RuntimeError(f\"Failed to load Voxtral model: {e}\")\n",
    "    \n",
    "    def _prepare_audio(\n",
    "        self,\n",
    "        audio: Union[AudioData, str, Path]  # Audio data, file path, or Path object to prepare\n",
    "    ) -> str:  # Returns path to the prepared audio file\n",
    "        \"\"\"Prepare audio for Voxtral processing.\"\"\"\n",
    "        if isinstance(audio, (str, Path)):\n",
    "            # Already a file path\n",
    "            return str(audio)\n",
    "        \n",
    "        elif isinstance(audio, AudioData):\n",
    "            # Save AudioData to temporary file\n",
    "            with tempfile.NamedTemporaryFile(suffix=\".wav\", delete=False) as tmp_file:\n",
    "                # Ensure audio is in the correct format\n",
    "                audio_array = audio.samples\n",
    "                \n",
    "                # If stereo, convert to mono\n",
    "                if audio_array.ndim > 1:\n",
    "                    audio_array = audio_array.mean(axis=1)\n",
    "                \n",
    "                # Ensure float32 and normalized\n",
    "                if audio_array.dtype != np.float32:\n",
    "                    audio_array = audio_array.astype(np.float32)\n",
    "                \n",
    "                # Normalize if needed\n",
    "                if audio_array.max() > 1.0:\n",
    "                    audio_array = audio_array / np.abs(audio_array).max()\n",
    "                \n",
    "                # Save to file\n",
    "                sf.write(tmp_file.name, audio_array, audio.sample_rate)\n",
    "                return tmp_file.name\n",
    "        else:\n",
    "            raise ValueError(f\"Unsupported audio input type: {type(audio)}\")\n",
    "    \n",
    "    def execute(\n",
    "        self,\n",
    "        audio: Union[AudioData, str, Path],  # Audio data or path to audio file to transcribe\n",
    "        **kwargs #  Additional arguments to override config\n",
    "    ) -> TranscriptionResult:  # Returns transcription result with text and metadata\n",
    "        \"\"\"Transcribe audio using Voxtral.\"\"\"\n",
    "        # Load model if not already loaded\n",
    "        self._load_model()\n",
    "        \n",
    "        # Prepare audio file\n",
    "        audio_path = self._prepare_audio(audio)\n",
    "        temp_file_created = not isinstance(audio, (str, Path))\n",
    "        \n",
    "        try:\n",
    "            # Merge runtime kwargs with config\n",
    "            exec_config = {**self.config, **kwargs}\n",
    "            \n",
    "            # Prepare inputs\n",
    "            self.logger.info(f\"Processing audio with Voxtral {exec_config['model_id']}\")\n",
    "            \n",
    "            inputs = self.processor.apply_transcription_request(\n",
    "                language=exec_config.get(\"language\", \"en\"),\n",
    "                audio=str(audio_path),\n",
    "                model_id=exec_config[\"model_id\"]\n",
    "            )\n",
    "            inputs = inputs.to(self.device, dtype=self.dtype)\n",
    "            \n",
    "            # Generation kwargs\n",
    "            generation_kwargs = {\n",
    "                \"max_new_tokens\": exec_config.get(\"max_new_tokens\", 25000),\n",
    "                \"do_sample\": exec_config.get(\"do_sample\", False),\n",
    "            }\n",
    "            \n",
    "            # Add sampling parameters if sampling is enabled\n",
    "            if generation_kwargs[\"do_sample\"]:\n",
    "                generation_kwargs[\"temperature\"] = exec_config.get(\"temperature\", 1.0)\n",
    "                generation_kwargs[\"top_p\"] = exec_config.get(\"top_p\", 0.95)\n",
    "            \n",
    "            # Generate transcription\n",
    "            with torch.no_grad():\n",
    "                with warnings.catch_warnings():\n",
    "                    warnings.simplefilter(\"ignore\")\n",
    "                    \n",
    "                    outputs = self.model.generate(\n",
    "                        **inputs,\n",
    "                        **generation_kwargs\n",
    "                    )\n",
    "            \n",
    "            # Decode the output\n",
    "            result_text = self.processor.batch_decode(\n",
    "                outputs[:, inputs.input_ids.shape[1]:], \n",
    "                skip_special_tokens=True\n",
    "            )[0]\n",
    "\n",
    "            # Clean up tensors immediately\n",
    "            del inputs\n",
    "            del outputs\n",
    "            \n",
    "            # Clear GPU cache if using CUDA\n",
    "            if self.device == \"cuda\" and torch.cuda.is_available():\n",
    "                torch.cuda.empty_cache()\n",
    "            \n",
    "            # Create transcription result\n",
    "            transcription_result = TranscriptionResult(\n",
    "                text=result_text.strip(),\n",
    "                confidence=None,  # Voxtral doesn't provide confidence scores\n",
    "                segments=None,  # Voxtral doesn't provide segments by default\n",
    "                metadata={\n",
    "                    \"model\": exec_config[\"model_id\"],\n",
    "                    \"language\": exec_config.get(\"language\", \"en\"),\n",
    "                    \"device\": self.device,\n",
    "                    \"dtype\": str(self.dtype),\n",
    "                }\n",
    "            )\n",
    "            \n",
    "            self.logger.info(f\"Transcription completed: {len(result_text.split())} words\")\n",
    "            return transcription_result\n",
    "            \n",
    "        finally:\n",
    "            # Clean up temporary file if created\n",
    "            if temp_file_created:\n",
    "                try:\n",
    "                    Path(audio_path).unlink()\n",
    "                except Exception:\n",
    "                    pass\n",
    "\n",
    "    def is_available(\n",
    "        self\n",
    "    ) -> bool:  # Returns True if Voxtral and its dependencies are available\n",
    "        \"\"\"Check if Voxtral is available.\"\"\"\n",
    "        return VOXTRAL_AVAILABLE\n",
    "    \n",
    "    def cleanup(\n",
    "        self\n",
    "    ) -> None:\n",
    "        \"\"\"Clean up resources with aggressive memory management.\"\"\"\n",
    "        if self.model is None and self.processor is None:\n",
    "            self.logger.info(\"No models to clean up\")\n",
    "            return\n",
    "        \n",
    "        self.logger.info(\"Unloading Voxtral model\")\n",
    "        \n",
    "        try:\n",
    "            # Move model to CPU first if it's on GPU (frees GPU memory immediately)\n",
    "            if self.model is not None and self.device == \"cuda\":\n",
    "                try:\n",
    "                    # Move to CPU to free GPU memory\n",
    "                    self.model = self.model.to('cpu')\n",
    "                    self.logger.debug(\"Model moved to CPU\")\n",
    "                except Exception as e:\n",
    "                    self.logger.warning(f\"Could not move model to CPU: {e}\")\n",
    "            \n",
    "            # Delete processor first (it may hold references to model components)\n",
    "            if self.processor is not None:\n",
    "                del self.processor\n",
    "                self.processor = None\n",
    "                self.logger.debug(\"Processor deleted\")\n",
    "            \n",
    "            # Delete model\n",
    "            if self.model is not None:\n",
    "                del self.model\n",
    "                self.model = None\n",
    "                self.logger.debug(\"Model deleted\")\n",
    "            \n",
    "            # Force garbage collection BEFORE GPU operations\n",
    "            import gc\n",
    "            gc.collect()\n",
    "            \n",
    "            # GPU-specific cleanup\n",
    "            if self.device == \"cuda\" and torch.cuda.is_available():\n",
    "                # Empty cache and synchronize\n",
    "                torch.cuda.empty_cache()\n",
    "                torch.cuda.synchronize()\n",
    "                \n",
    "                # Optional: more aggressive cleanup\n",
    "                torch.cuda.ipc_collect()\n",
    "                \n",
    "                # Log memory stats\n",
    "                if torch.cuda.is_available():\n",
    "                    allocated = torch.cuda.memory_allocated() / 1024**3\n",
    "                    reserved = torch.cuda.memory_reserved() / 1024**3\n",
    "                    self.logger.info(f\"GPU memory after cleanup - Allocated: {allocated:.2f}GB, Reserved: {reserved:.2f}GB\")\n",
    "            \n",
    "            self.logger.info(\"Cleanup completed successfully\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"Error during cleanup: {e}\")\n",
    "            # Ensure references are cleared even if cleanup fails\n",
    "            self.model = None\n",
    "            self.processor = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f195e29",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "@patch\n",
    "def supports_streaming(\n",
    "    self:VoxtralHFPlugin\n",
    ") -> bool:\n",
    "    \"\"\"Check if this plugin supports streaming transcription.\"\"\"\n",
    "    return True\n",
    "\n",
    "@patch\n",
    "def execute_stream(\n",
    "    self:VoxtralHFPlugin,\n",
    "    audio: Union[AudioData, str, Path],  # Audio data or path to audio file\n",
    "    **kwargs  # Additional plugin-specific parameters\n",
    ") -> Generator[str, None, TranscriptionResult]:  # Yields text chunks, returns final result\n",
    "    \"\"\"Stream transcription results chunk by chunk.\n",
    "    \n",
    "    Args:\n",
    "        audio: Audio data or path to audio file\n",
    "        **kwargs: Additional plugin-specific parameters\n",
    "        \n",
    "    Yields:\n",
    "        str: Partial transcription text chunks as they become available\n",
    "        \n",
    "    Returns:\n",
    "        TranscriptionResult: Final complete transcription with metadata\n",
    "    \"\"\"\n",
    "    # Load model if not already loaded\n",
    "    self._load_model()\n",
    "    \n",
    "    # Prepare audio file\n",
    "    audio_path = self._prepare_audio(audio)\n",
    "    temp_file_created = not isinstance(audio, (str, Path))\n",
    "    \n",
    "    try:\n",
    "        # Merge runtime kwargs with config\n",
    "        exec_config = {**self.config, **kwargs}\n",
    "        \n",
    "        # Prepare inputs\n",
    "        self.logger.info(f\"Streaming transcription with Voxtral {exec_config['model_id']}\")\n",
    "        \n",
    "        inputs = self.processor.apply_transcription_request(\n",
    "            language=exec_config.get(\"language\", \"en\"),\n",
    "            audio=str(audio_path),\n",
    "            model_id=exec_config[\"model_id\"]\n",
    "        )\n",
    "        inputs = inputs.to(self.device, dtype=self.dtype)\n",
    "        \n",
    "        # Create streamer\n",
    "        from transformers import TextIteratorStreamer\n",
    "        streamer = TextIteratorStreamer(\n",
    "            self.processor.tokenizer, \n",
    "            skip_prompt=True, \n",
    "            skip_special_tokens=True\n",
    "        )\n",
    "        \n",
    "        # Generation kwargs\n",
    "        generation_kwargs = {\n",
    "            **inputs,\n",
    "            \"max_new_tokens\": exec_config.get(\"max_new_tokens\", 25000),\n",
    "            \"do_sample\": exec_config.get(\"do_sample\", False),\n",
    "            \"streamer\": streamer,\n",
    "        }\n",
    "        \n",
    "        # Add sampling parameters if sampling is enabled\n",
    "        if generation_kwargs[\"do_sample\"]:\n",
    "            generation_kwargs[\"temperature\"] = exec_config.get(\"temperature\", 1.0)\n",
    "            generation_kwargs[\"top_p\"] = exec_config.get(\"top_p\", 0.95)\n",
    "        \n",
    "        # Start generation in a separate thread with torch.no_grad()               \n",
    "        def generate_with_no_grad():                                               \n",
    "          with torch.no_grad():                                                  \n",
    "              self.model.generate(**generation_kwargs)                           \n",
    "        \n",
    "        thread = Thread(target=generate_with_no_grad)                              \n",
    "        thread.start() \n",
    "        \n",
    "        # Collect generated text\n",
    "        generated_text = \"\"\n",
    "        for text_chunk in streamer:\n",
    "            generated_text += text_chunk\n",
    "            yield text_chunk\n",
    "        \n",
    "        # Wait for generation to complete\n",
    "        thread.join()\n",
    "\n",
    "        # Clean up tensors immediately\n",
    "        del inputs\n",
    "        \n",
    "        # Clear GPU cache if using CUDA\n",
    "        if self.device == \"cuda\" and torch.cuda.is_available():\n",
    "            torch.cuda.empty_cache()\n",
    "        \n",
    "        # Return final result\n",
    "        return TranscriptionResult(\n",
    "            text=generated_text.strip(),\n",
    "            confidence=None,\n",
    "            segments=None,\n",
    "            metadata={\n",
    "                \"model\": exec_config[\"model_id\"],\n",
    "                \"language\": exec_config.get(\"language\", \"en\"),\n",
    "                \"device\": self.device,\n",
    "                \"dtype\": str(self.dtype),\n",
    "                \"streaming\": True,\n",
    "            }\n",
    "        )\n",
    "        \n",
    "    finally:\n",
    "        # Clean up temporary file if created\n",
    "        if temp_file_created:\n",
    "            try:\n",
    "                Path(audio_path).unlink()\n",
    "            except Exception:\n",
    "                pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f956ac0",
   "metadata": {},
   "source": [
    "## Testing the Plugin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "521bac38",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Voxtral available: True\n",
      "Plugin name: voxtral_hf\n",
      "Plugin version: 1.0.0\n",
      "Supported formats: ['wav', 'mp3', 'flac', 'm4a', 'ogg', 'webm', 'mp4', 'avi', 'mov']\n",
      "Supports streaming: True\n"
     ]
    }
   ],
   "source": [
    "# Test basic functionality\n",
    "plugin = VoxtralHFPlugin()\n",
    "\n",
    "# Check availability\n",
    "print(f\"Voxtral available: {plugin.is_available()}\")\n",
    "print(f\"Plugin name: {plugin.name}\")\n",
    "print(f\"Plugin version: {plugin.version}\")\n",
    "print(f\"Supported formats: {plugin.supported_formats}\")\n",
    "print(f\"Supports streaming: {plugin.supports_streaming()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76e26b64",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Available models:\n",
      "  - mistralai/Voxtral-Mini-3B-2507\n",
      "  - mistralai/Voxtral-Small-24B-2507\n"
     ]
    }
   ],
   "source": [
    "# Test configuration schema\n",
    "schema = plugin.get_config_schema()\n",
    "print(\"Available models:\")\n",
    "for model in schema[\"properties\"][\"model_id\"][\"enum\"]:\n",
    "    print(f\"  - {model}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f446f47",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Valid config: Valid=True\n",
      "Invalid model: Valid=False\n",
      "  Error: 'invalid_model' is not one of ['mistralai/Voxtral-Mini-3B-2507', 'mistralai/Voxtral-Small-24B-2507']\n",
      "Temperature out of range: Valid=False\n",
      "  Error: 'model_id' is a required property\n",
      "\n",
      "Failed validating 'required' in schema:\n",
      "    {'$schema': 'http://j\n"
     ]
    }
   ],
   "source": [
    "# Test configuration validation\n",
    "test_configs = [\n",
    "    ({\"model_id\": \"mistralai/Voxtral-Mini-3B-2507\"}, \"Valid config\"),\n",
    "    ({\"model_id\": \"invalid_model\"}, \"Invalid model\"),\n",
    "    ({\"temperature\": 2.5}, \"Temperature out of range\"),\n",
    "]\n",
    "\n",
    "for config, description in test_configs:\n",
    "    is_valid, error = plugin.validate_config(config)\n",
    "    print(f\"{description}: Valid={is_valid}\")\n",
    "    if error:\n",
    "        print(f\"  Error: {error[:100]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed1b6bd3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current config: mistralai/Voxtral-Mini-3B-2507\n"
     ]
    }
   ],
   "source": [
    "# Test initialization\n",
    "plugin.initialize({\"model_id\": \"mistralai/Voxtral-Mini-3B-2507\", \"device\": \"cpu\"})\n",
    "print(f\"Current config: {plugin.get_current_config()['model_id']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84faec63",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "import nbdev; nbdev.nbdev_export()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
