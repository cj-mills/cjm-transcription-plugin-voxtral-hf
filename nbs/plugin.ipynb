{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "37880f73",
   "metadata": {},
   "source": [
    "# Voxtral HF Plugin\n",
    "\n",
    "> Plugin implementation for Mistral Voxtral transcription through Hugging Face Transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e9838a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp plugin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96837562",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "from nbdev.showdoc import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2fbcf2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "import sqlite3\n",
    "import json\n",
    "import time\n",
    "import os\n",
    "import sys\n",
    "from uuid import uuid4\n",
    "import logging\n",
    "from pathlib import Path\n",
    "from dataclasses import dataclass, field\n",
    "from dataclasses import replace as dataclass_replace\n",
    "from typing import Dict, Any, Optional, List, Union, Generator\n",
    "import tempfile\n",
    "import warnings\n",
    "from threading import Thread\n",
    "\n",
    "from fastcore.basics import patch\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import soundfile as sf\n",
    "\n",
    "try:\n",
    "    from transformers import VoxtralForConditionalGeneration, AutoProcessor\n",
    "    from transformers import TextStreamer, TextIteratorStreamer\n",
    "    VOXTRAL_AVAILABLE = True\n",
    "except ImportError:\n",
    "    VOXTRAL_AVAILABLE = False\n",
    "    \n",
    "from cjm_transcription_plugin_system.plugin_interface import TranscriptionPlugin\n",
    "from cjm_transcription_plugin_system.core import AudioData, TranscriptionResult\n",
    "from cjm_plugin_system.utils.validation import (\n",
    "    dict_to_config, config_to_dict, validate_config, dataclass_to_jsonschema,\n",
    "    SCHEMA_TITLE, SCHEMA_DESC, SCHEMA_MIN, SCHEMA_MAX, SCHEMA_ENUM\n",
    ")\n",
    "from cjm_transcription_plugin_voxtral_hf.meta import (\n",
    "    get_plugin_metadata\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28e027a7-577a-41ac-8f13-bf84a68b5adb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "@dataclass\n",
    "class VoxtralHFPluginConfig:\n",
    "    \"\"\"Configuration for Voxtral HF transcription plugin.\"\"\"\n",
    "    model_id:str = field(\n",
    "        default=\"mistralai/Voxtral-Mini-3B-2507\",\n",
    "        metadata={\n",
    "            SCHEMA_TITLE: \"Model ID\",\n",
    "            SCHEMA_DESC: \"Voxtral model to use. Mini is faster, Small is more accurate.\",\n",
    "            SCHEMA_ENUM: [\"mistralai/Voxtral-Mini-3B-2507\", \"mistralai/Voxtral-Small-24B-2507\"]\n",
    "        }\n",
    "    )\n",
    "    device:str = field(\n",
    "        default=\"auto\",\n",
    "        metadata={\n",
    "            SCHEMA_TITLE: \"Device\",\n",
    "            SCHEMA_DESC: \"Device for inference (auto will use CUDA if available)\",\n",
    "            SCHEMA_ENUM: [\"auto\", \"cpu\", \"cuda\"]\n",
    "        }\n",
    "    )\n",
    "    dtype:str = field(\n",
    "        default=\"auto\",\n",
    "        metadata={\n",
    "            SCHEMA_TITLE: \"Data Type\",\n",
    "            SCHEMA_DESC: \"Data type for model weights (auto will use bfloat16 on GPU, float32 on CPU)\",\n",
    "            SCHEMA_ENUM: [\"auto\", \"bfloat16\", \"float16\", \"float32\"]\n",
    "        }\n",
    "    )\n",
    "    language:Optional[str] = field(\n",
    "        default=\"en\",\n",
    "        metadata={\n",
    "            SCHEMA_TITLE: \"Language\",\n",
    "            SCHEMA_DESC: \"Language code for transcription (e.g., 'en', 'es', 'fr')\"\n",
    "        }\n",
    "    )\n",
    "    max_new_tokens:int = field(\n",
    "        default=25000,\n",
    "        metadata={\n",
    "            SCHEMA_TITLE: \"Max New Tokens\",\n",
    "            SCHEMA_DESC: \"Maximum number of tokens to generate\",\n",
    "            SCHEMA_MIN: 1,\n",
    "            SCHEMA_MAX: 50000\n",
    "        }\n",
    "    )\n",
    "    do_sample:bool = field(\n",
    "        default=False,\n",
    "        metadata={\n",
    "            SCHEMA_TITLE: \"Do Sample\",\n",
    "            SCHEMA_DESC: \"Whether to use sampling (true) or greedy decoding (False)\"\n",
    "        }\n",
    "    )\n",
    "    temperature:float = field(\n",
    "        default=1.0,\n",
    "        metadata={\n",
    "            SCHEMA_TITLE: \"Temperature\",\n",
    "            SCHEMA_DESC: \"Temperature for sampling (only used when do_sample=true)\",\n",
    "            SCHEMA_MIN: 0.0,\n",
    "            SCHEMA_MAX: 2.0\n",
    "        }\n",
    "    )\n",
    "    top_p:float = field(\n",
    "        default=0.95,\n",
    "        metadata={\n",
    "            SCHEMA_TITLE: \"Top P\",\n",
    "            SCHEMA_DESC: \"Top-p (nucleus) sampling parameter (only used when do_sample=true)\",\n",
    "            SCHEMA_MIN: 0.0,\n",
    "            SCHEMA_MAX: 1.0\n",
    "        }\n",
    "    )\n",
    "    streaming:bool = field(\n",
    "        default=False,\n",
    "        metadata={\n",
    "            SCHEMA_TITLE: \"Streaming\",\n",
    "            SCHEMA_DESC: \"Enable streaming output (yields partial results as they're generated)\"\n",
    "        }\n",
    "    )\n",
    "    trust_remote_code:bool = field(\n",
    "        default=False,\n",
    "        metadata={\n",
    "            SCHEMA_TITLE: \"Trust Remote Code\",\n",
    "            SCHEMA_DESC: \"Whether to trust remote code when loading the model\"\n",
    "        }\n",
    "    )\n",
    "    cache_dir:Optional[str] = field(\n",
    "        default=None,\n",
    "        metadata={\n",
    "            SCHEMA_TITLE: \"Cache Directory\",\n",
    "            SCHEMA_DESC: \"Directory to cache downloaded models\"\n",
    "        }\n",
    "    )\n",
    "    compile_model:bool = field(\n",
    "        default=False,\n",
    "        metadata={\n",
    "            SCHEMA_TITLE: \"Compile Model\",\n",
    "            SCHEMA_DESC: \"Use torch.compile for potential speedup (requires PyTorch 2.0+)\"\n",
    "        }\n",
    "    )\n",
    "    load_in_8bit:bool = field(\n",
    "        default=False,\n",
    "        metadata={\n",
    "            SCHEMA_TITLE: \"Load in 8-bit\",\n",
    "            SCHEMA_DESC: \"Load model in 8-bit quantization (requires bitsandbytes)\"\n",
    "        }\n",
    "    )\n",
    "    load_in_4bit:bool = field(\n",
    "        default=False,\n",
    "        metadata={\n",
    "            SCHEMA_TITLE: \"Load in 4-bit\",\n",
    "            SCHEMA_DESC: \"Load model in 4-bit quantization (requires bitsandbytes)\"\n",
    "        }\n",
    "    )\n",
    "\n",
    "\n",
    "class VoxtralHFPlugin(TranscriptionPlugin):\n",
    "    \"\"\"Mistral Voxtral transcription plugin via Hugging Face Transformers.\"\"\"\n",
    "    \n",
    "    config_class = VoxtralHFPluginConfig\n",
    "    \n",
    "    def __init__(self):\n",
    "        \"\"\"Initialize the Voxtral HF plugin with default configuration.\"\"\"\n",
    "        self.logger = logging.getLogger(f\"{__name__}.{type(self).__name__}\")\n",
    "        self.config: VoxtralHFPluginConfig = None\n",
    "        self.model = None\n",
    "        self.processor = None\n",
    "        self.device = None\n",
    "        self.dtype = None\n",
    "    \n",
    "    @property\n",
    "    def name(self) -> str: # Plugin name identifier\n",
    "        \"\"\"Get the plugin name identifier.\"\"\"\n",
    "        return \"voxtral_hf\"\n",
    "    \n",
    "    @property\n",
    "    def version(self) -> str: # Plugin version string\n",
    "        \"\"\"Get the plugin version string.\"\"\"\n",
    "        return \"1.0.0\"\n",
    "    \n",
    "    @property\n",
    "    def supported_formats(self) -> List[str]: # List of supported audio formats\n",
    "        \"\"\"Get the list of supported audio file formats.\"\"\"\n",
    "        return [\"wav\", \"mp3\", \"flac\", \"m4a\", \"ogg\", \"webm\", \"mp4\", \"avi\", \"mov\"]\n",
    "\n",
    "    def get_current_config(self) -> Dict[str, Any]: # Current configuration as dictionary\n",
    "        \"\"\"Return current configuration state.\"\"\"\n",
    "        if not self.config:\n",
    "            return {}\n",
    "        return config_to_dict(self.config)\n",
    "\n",
    "    def get_config_schema(self) -> Dict[str, Any]: # JSON Schema for configuration\n",
    "        \"\"\"Return JSON Schema for UI generation.\"\"\"\n",
    "        return dataclass_to_jsonschema(VoxtralHFPluginConfig)\n",
    "\n",
    "    @staticmethod\n",
    "    def get_config_dataclass() -> VoxtralHFPluginConfig: # Configuration dataclass\n",
    "        \"\"\"Return dataclass describing the plugin's configuration options.\"\"\"\n",
    "        return VoxtralHFPluginConfig\n",
    "    \n",
    "    def initialize(\n",
    "        self,\n",
    "        config: Optional[Any] = None # Configuration dataclass, dict, or None\n",
    "    ) -> None:\n",
    "        \"\"\"Initialize or re-configure the plugin (idempotent).\"\"\"\n",
    "        # Parse new config\n",
    "        new_config = dict_to_config(VoxtralHFPluginConfig, config or {})\n",
    "        \n",
    "        # Check for changes if already running\n",
    "        if self.config:\n",
    "            # If the model changed, unload old model\n",
    "            if self.config.model_id != new_config.model_id:\n",
    "                self.logger.info(f\"Config change: Model {self.config.model_id} -> {new_config.model_id}\")\n",
    "                self._unload_model()\n",
    "            \n",
    "            # If device changed, unload\n",
    "            if self.config.device != new_config.device:\n",
    "                self.logger.info(f\"Config change: Device {self.config.device} -> {new_config.device}\")\n",
    "                self._unload_model()\n",
    "            \n",
    "            # If dtype changed, unload\n",
    "            if self.config.dtype != new_config.dtype:\n",
    "                self.logger.info(f\"Config change: Dtype {self.config.dtype} -> {new_config.dtype}\")\n",
    "                self._unload_model()\n",
    "            \n",
    "            # If quantization settings changed, unload\n",
    "            if (self.config.load_in_8bit != new_config.load_in_8bit or\n",
    "                self.config.load_in_4bit != new_config.load_in_4bit):\n",
    "                self.logger.info(\"Config change: Quantization settings changed\")\n",
    "                self._unload_model()\n",
    "        \n",
    "        # Apply new config\n",
    "        self.config = new_config\n",
    "        \n",
    "        # Set device\n",
    "        if self.config.device == \"auto\":\n",
    "            self.device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "        else:\n",
    "            self.device = self.config.device\n",
    "        \n",
    "        # Set dtype\n",
    "        if self.config.dtype == \"auto\":\n",
    "            if self.device == \"cuda\":\n",
    "                self.dtype = torch.bfloat16\n",
    "            else:\n",
    "                self.dtype = torch.float32\n",
    "        else:\n",
    "            dtype_map = {\n",
    "                \"bfloat16\": torch.bfloat16,\n",
    "                \"float16\": torch.float16,\n",
    "                \"float32\": torch.float32\n",
    "            }\n",
    "            self.dtype = dtype_map[self.config.dtype]\n",
    "        \n",
    "        self.logger.info(f\"Initialized Voxtral HF plugin with model '{self.config.model_id}' on device '{self.device}' with dtype '{self.dtype}'\")\n",
    "    \n",
    "    def _unload_model(self) -> None:\n",
    "        \"\"\"Unload the current model and free resources.\"\"\"\n",
    "        if self.model is None and self.processor is None:\n",
    "            return\n",
    "        \n",
    "        self.logger.info(\"Unloading Voxtral model for reconfiguration\")\n",
    "        \n",
    "        try:\n",
    "            # Move model to CPU first if it's on GPU\n",
    "            if self.model is not None and self.device == \"cuda\":\n",
    "                try:\n",
    "                    self.model = self.model.to('cpu')\n",
    "                except Exception as e:\n",
    "                    self.logger.warning(f\"Could not move model to CPU: {e}\")\n",
    "            \n",
    "            # Delete processor and model\n",
    "            if self.processor is not None:\n",
    "                del self.processor\n",
    "                self.processor = None\n",
    "            \n",
    "            if self.model is not None:\n",
    "                del self.model\n",
    "                self.model = None\n",
    "            \n",
    "            # Force garbage collection\n",
    "            import gc\n",
    "            gc.collect()\n",
    "            \n",
    "            # GPU-specific cleanup\n",
    "            if self.device == \"cuda\" and torch.cuda.is_available():\n",
    "                torch.cuda.empty_cache()\n",
    "                torch.cuda.synchronize()\n",
    "                \n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"Error during model unload: {e}\")\n",
    "            self.model = None\n",
    "            self.processor = None\n",
    "    \n",
    "    def _load_model(self) -> None:\n",
    "        \"\"\"Load the Voxtral model and processor (lazy loading).\"\"\"\n",
    "        if self.model is None or self.processor is None:\n",
    "            try:\n",
    "                self.logger.info(f\"Loading Voxtral model: {self.config.model_id}\")\n",
    "                \n",
    "                # Load processor\n",
    "                self.processor = AutoProcessor.from_pretrained(\n",
    "                    self.config.model_id,\n",
    "                    cache_dir=self.config.cache_dir,\n",
    "                    trust_remote_code=self.config.trust_remote_code\n",
    "                )\n",
    "                \n",
    "                # Model loading kwargs\n",
    "                model_kwargs = {\n",
    "                    \"cache_dir\": self.config.cache_dir,\n",
    "                    \"trust_remote_code\": self.config.trust_remote_code,\n",
    "                    \"device_map\": self.device,\n",
    "                }\n",
    "                \n",
    "                # Add quantization settings if specified\n",
    "                if self.config.load_in_8bit:\n",
    "                    model_kwargs[\"load_in_8bit\"] = True\n",
    "                elif self.config.load_in_4bit:\n",
    "                    model_kwargs[\"load_in_4bit\"] = True\n",
    "                else:\n",
    "                    model_kwargs[\"dtype\"] = self.dtype\n",
    "                \n",
    "                # Load model\n",
    "                self.model = VoxtralForConditionalGeneration.from_pretrained(\n",
    "                    self.config.model_id,\n",
    "                    **model_kwargs\n",
    "                )\n",
    "                \n",
    "                # Optionally compile the model (PyTorch 2.0+)\n",
    "                if self.config.compile_model and hasattr(torch, 'compile'):\n",
    "                    self.model = torch.compile(self.model)\n",
    "                    self.logger.info(\"Model compiled with torch.compile\")\n",
    "                    \n",
    "                self.logger.info(\"Voxtral model loaded successfully\")\n",
    "            except Exception as e:\n",
    "                raise RuntimeError(f\"Failed to load Voxtral model: {e}\")\n",
    "    \n",
    "    def _prepare_audio(\n",
    "        self,\n",
    "        audio: Union[AudioData, str, Path] # Audio data, file path, or Path object to prepare\n",
    "    ) -> str: # Path to the prepared audio file\n",
    "        \"\"\"Prepare audio for Voxtral processing.\"\"\"\n",
    "        if isinstance(audio, (str, Path)):\n",
    "            # Already a file path\n",
    "            return str(audio)\n",
    "        \n",
    "        elif isinstance(audio, AudioData):\n",
    "            # Save AudioData to temporary file\n",
    "            with tempfile.NamedTemporaryFile(suffix=\".wav\", delete=False) as tmp_file:\n",
    "                # Ensure audio is in the correct format\n",
    "                audio_array = audio.samples\n",
    "                \n",
    "                # If stereo, convert to mono\n",
    "                if audio_array.ndim > 1:\n",
    "                    audio_array = audio_array.mean(axis=1)\n",
    "                \n",
    "                # Ensure float32 and normalized\n",
    "                if audio_array.dtype != np.float32:\n",
    "                    audio_array = audio_array.astype(np.float32)\n",
    "                \n",
    "                # Normalize if needed\n",
    "                if audio_array.max() > 1.0:\n",
    "                    audio_array = audio_array / np.abs(audio_array).max()\n",
    "                \n",
    "                # Save to file\n",
    "                sf.write(tmp_file.name, audio_array, audio.sample_rate)\n",
    "                return tmp_file.name\n",
    "        else:\n",
    "            raise ValueError(f\"Unsupported audio input type: {type(audio)}\")\n",
    "\n",
    "    def _init_db(self):\n",
    "        \"\"\"Ensure table exists.\"\"\"\n",
    "        db_path = get_plugin_metadata()[\"db_path\"]\n",
    "        with sqlite3.connect(db_path) as con:\n",
    "            con.execute(\"\"\"\n",
    "                CREATE TABLE IF NOT EXISTS transcriptions (\n",
    "                    id INTEGER PRIMARY KEY AUTOINCREMENT,\n",
    "                    job_id TEXT,\n",
    "                    audio_path TEXT,\n",
    "                    text TEXT,\n",
    "                    segments JSON,\n",
    "                    metadata JSON,\n",
    "                    created_at REAL\n",
    "                )\n",
    "            \"\"\")\n",
    "            con.execute(\"CREATE INDEX IF NOT EXISTS idx_job_id ON transcriptions(job_id)\")\n",
    "\n",
    "    def _save_to_db(self, result: TranscriptionResult, audio_path: str, **kwargs) -> None:\n",
    "        \"\"\"Save result to SQLite.\"\"\"\n",
    "        try:\n",
    "            self._init_db()\n",
    "            db_path = get_plugin_metadata()[\"db_path\"]\n",
    "            \n",
    "            # Extract a job_id if provided, else gen random\n",
    "            job_id = kwargs.get(\"job_id\", str(uuid4()))\n",
    "            \n",
    "            # Serialize complex objects\n",
    "            segments_json = json.dumps(result.segments) if result.segments else None\n",
    "            metadata_json = json.dumps(result.metadata)\n",
    "            \n",
    "            with sqlite3.connect(db_path) as con:\n",
    "                con.execute(\n",
    "                    \"\"\"\n",
    "                    INSERT INTO transcriptions \n",
    "                    (job_id, audio_path, text, segments, metadata, created_at)\n",
    "                    VALUES (?, ?, ?, ?, ?, ?)\n",
    "                    \"\"\",\n",
    "                    (job_id, str(audio_path), result.text, segments_json, metadata_json, time.time())\n",
    "                )\n",
    "                self.logger.info(f\"Saved result to DB (Job: {job_id})\")\n",
    "                \n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"Failed to save to DB: {e}\")\n",
    "    \n",
    "    def execute(\n",
    "        self,\n",
    "        audio: Union[AudioData, str, Path], # Audio data or path to audio file to transcribe\n",
    "        **kwargs # Additional arguments to override config\n",
    "    ) -> TranscriptionResult: # Transcription result with text and metadata\n",
    "        \"\"\"Transcribe audio using Voxtral.\"\"\"\n",
    "        # Load model if not already loaded\n",
    "        self._load_model()\n",
    "        \n",
    "        # Prepare audio file\n",
    "        audio_path = self._prepare_audio(audio)\n",
    "        temp_file_created = not isinstance(audio, (str, Path))\n",
    "        \n",
    "        try:\n",
    "            # Get config values, allowing kwargs overrides\n",
    "            model_id = kwargs.get(\"model_id\", self.config.model_id)\n",
    "            language = kwargs.get(\"language\", self.config.language)\n",
    "            max_new_tokens = kwargs.get(\"max_new_tokens\", self.config.max_new_tokens)\n",
    "            do_sample = kwargs.get(\"do_sample\", self.config.do_sample)\n",
    "            temperature = kwargs.get(\"temperature\", self.config.temperature)\n",
    "            top_p = kwargs.get(\"top_p\", self.config.top_p)\n",
    "            \n",
    "            # Prepare inputs\n",
    "            self.logger.info(f\"Processing audio with Voxtral {model_id}\")\n",
    "            \n",
    "            inputs = self.processor.apply_transcription_request(\n",
    "                language=language or \"en\",\n",
    "                audio=str(audio_path),\n",
    "                model_id=model_id\n",
    "            )\n",
    "            inputs = inputs.to(self.device, dtype=self.dtype)\n",
    "            \n",
    "            # Generation kwargs\n",
    "            generation_kwargs = {\n",
    "                \"max_new_tokens\": max_new_tokens,\n",
    "                \"do_sample\": do_sample,\n",
    "            }\n",
    "            \n",
    "            # Add sampling parameters if sampling is enabled\n",
    "            if do_sample:\n",
    "                generation_kwargs[\"temperature\"] = temperature\n",
    "                generation_kwargs[\"top_p\"] = top_p\n",
    "            \n",
    "            # Generate transcription\n",
    "            with torch.no_grad():\n",
    "                with warnings.catch_warnings():\n",
    "                    warnings.simplefilter(\"ignore\")\n",
    "                    \n",
    "                    outputs = self.model.generate(\n",
    "                        **inputs,\n",
    "                        **generation_kwargs\n",
    "                    )\n",
    "            \n",
    "            # Decode the output\n",
    "            result_text = self.processor.batch_decode(\n",
    "                outputs[:, inputs.input_ids.shape[1]:], \n",
    "                skip_special_tokens=True\n",
    "            )[0]\n",
    "\n",
    "            # Clean up tensors immediately\n",
    "            del inputs\n",
    "            del outputs\n",
    "            \n",
    "            # Clear GPU cache if using CUDA\n",
    "            if self.device == \"cuda\" and torch.cuda.is_available():\n",
    "                torch.cuda.empty_cache()\n",
    "\n",
    "            # Capture provenance metadata passed via kwargs\n",
    "            provenance_meta = {\n",
    "                k: v for k, v in kwargs.items() \n",
    "                if k in ['source_start_time', 'source_end_time']\n",
    "            }\n",
    "            \n",
    "            # Create transcription result\n",
    "            transcription_result = TranscriptionResult(\n",
    "                text=result_text.strip(),\n",
    "                confidence=None,  # Voxtral doesn't provide confidence scores\n",
    "                segments=None,  # Voxtral doesn't provide segments by default\n",
    "                metadata={\n",
    "                    \"model\": model_id,\n",
    "                    **provenance_meta,\n",
    "                    \"language\": language or \"en\",\n",
    "                    \"device\": self.device,\n",
    "                    \"dtype\": str(self.dtype),\n",
    "                }\n",
    "            )\n",
    "\n",
    "            # Capture original path for DB\n",
    "            original_path = str(audio)\n",
    "            if hasattr(audio, 'to_temp_file'): original_path = \"in_memory_data\"\n",
    "            \n",
    "            # Save to database\n",
    "            self._save_to_db(transcription_result, original_path, **kwargs)\n",
    "            \n",
    "            self.logger.info(f\"Transcription completed: {len(result_text.split())} words\")\n",
    "            return transcription_result\n",
    "            \n",
    "        finally:\n",
    "            # Clean up temporary file if created\n",
    "            if temp_file_created:\n",
    "                try:\n",
    "                    Path(audio_path).unlink()\n",
    "                except Exception:\n",
    "                    pass\n",
    "\n",
    "    def is_available(self) -> bool: # True if Voxtral and its dependencies are available\n",
    "        \"\"\"Check if Voxtral is available.\"\"\"\n",
    "        return VOXTRAL_AVAILABLE\n",
    "    \n",
    "    def cleanup(self) -> None:\n",
    "        \"\"\"Clean up resources with aggressive memory management.\"\"\"\n",
    "        if self.model is None and self.processor is None:\n",
    "            self.logger.info(\"No models to clean up\")\n",
    "            return\n",
    "        \n",
    "        self.logger.info(\"Unloading Voxtral model\")\n",
    "        \n",
    "        try:\n",
    "            # Move model to CPU first if it's on GPU (frees GPU memory immediately)\n",
    "            if self.model is not None and self.device == \"cuda\":\n",
    "                try:\n",
    "                    # Move to CPU to free GPU memory\n",
    "                    self.model = self.model.to('cpu')\n",
    "                    self.logger.debug(\"Model moved to CPU\")\n",
    "                except Exception as e:\n",
    "                    self.logger.warning(f\"Could not move model to CPU: {e}\")\n",
    "            \n",
    "            # Delete processor first (it may hold references to model components)\n",
    "            if self.processor is not None:\n",
    "                del self.processor\n",
    "                self.processor = None\n",
    "                self.logger.debug(\"Processor deleted\")\n",
    "            \n",
    "            # Delete model\n",
    "            if self.model is not None:\n",
    "                del self.model\n",
    "                self.model = None\n",
    "                self.logger.debug(\"Model deleted\")\n",
    "            \n",
    "            # Force garbage collection BEFORE GPU operations\n",
    "            import gc\n",
    "            gc.collect()\n",
    "            \n",
    "            # GPU-specific cleanup\n",
    "            if self.device == \"cuda\" and torch.cuda.is_available():\n",
    "                # Empty cache and synchronize\n",
    "                torch.cuda.empty_cache()\n",
    "                torch.cuda.synchronize()\n",
    "                \n",
    "                # Optional: more aggressive cleanup\n",
    "                torch.cuda.ipc_collect()\n",
    "                \n",
    "                # Log memory stats\n",
    "                if torch.cuda.is_available():\n",
    "                    allocated = torch.cuda.memory_allocated() / 1024**3\n",
    "                    reserved = torch.cuda.memory_reserved() / 1024**3\n",
    "                    self.logger.info(f\"GPU memory after cleanup - Allocated: {allocated:.2f}GB, Reserved: {reserved:.2f}GB\")\n",
    "            \n",
    "            self.logger.info(\"Cleanup completed successfully\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"Error during cleanup: {e}\")\n",
    "            # Ensure references are cleared even if cleanup fails\n",
    "            self.model = None\n",
    "            self.processor = None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "pwqaaty6acs",
   "metadata": {},
   "source": [
    "## Streaming Support\n",
    "\n",
    "The `execute_stream` method provides real-time transcription output:\n",
    "\n",
    "- **Yields**: Partial transcription text chunks as they become available during generation\n",
    "- **Returns**: Final `TranscriptionResult` with complete text and metadata after streaming completes\n",
    "- **Parameters**: Same as `execute()` - accepts `AudioData`, file path string, or `Path` object, plus optional kwargs to override config\n",
    "- **Usage**: Iterate over the generator to receive text chunks, then access the return value for the final result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f195e29",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "@patch\n",
    "def supports_streaming(\n",
    "    self:VoxtralHFPlugin\n",
    ") -> bool:  # True if streaming is supported\n",
    "    \"\"\"Check if this plugin supports streaming transcription.\"\"\"\n",
    "    return True\n",
    "\n",
    "@patch\n",
    "def execute_stream(\n",
    "    self:VoxtralHFPlugin,\n",
    "    audio: Union[AudioData, str, Path],  # Audio data or path to audio file\n",
    "    **kwargs  # Additional plugin-specific parameters\n",
    ") -> Generator[str, None, TranscriptionResult]:  # Yields text chunks, returns final result\n",
    "    \"\"\"Stream transcription results chunk by chunk.\"\"\"\n",
    "    # Load model if not already loaded\n",
    "    self._load_model()\n",
    "    \n",
    "    # Prepare audio file\n",
    "    audio_path = self._prepare_audio(audio)\n",
    "    temp_file_created = not isinstance(audio, (str, Path))\n",
    "    \n",
    "    try:\n",
    "        # Get config values, allowing kwargs overrides\n",
    "        model_id = kwargs.get(\"model_id\", self.config.model_id)\n",
    "        language = kwargs.get(\"language\", self.config.language)\n",
    "        max_new_tokens = kwargs.get(\"max_new_tokens\", self.config.max_new_tokens)\n",
    "        do_sample = kwargs.get(\"do_sample\", self.config.do_sample)\n",
    "        temperature = kwargs.get(\"temperature\", self.config.temperature)\n",
    "        top_p = kwargs.get(\"top_p\", self.config.top_p)\n",
    "        \n",
    "        # Prepare inputs\n",
    "        self.logger.info(f\"Streaming transcription with Voxtral {model_id}\")\n",
    "        \n",
    "        inputs = self.processor.apply_transcription_request(\n",
    "            language=language or \"en\",\n",
    "            audio=str(audio_path),\n",
    "            model_id=model_id\n",
    "        )\n",
    "        inputs = inputs.to(self.device, dtype=self.dtype)\n",
    "        \n",
    "        # Create streamer\n",
    "        from transformers import TextIteratorStreamer\n",
    "        streamer = TextIteratorStreamer(\n",
    "            self.processor.tokenizer, \n",
    "            skip_prompt=True, \n",
    "            skip_special_tokens=True\n",
    "        )\n",
    "        \n",
    "        # Generation kwargs\n",
    "        generation_kwargs = {\n",
    "            **inputs,\n",
    "            \"max_new_tokens\": max_new_tokens,\n",
    "            \"do_sample\": do_sample,\n",
    "            \"streamer\": streamer,\n",
    "        }\n",
    "        \n",
    "        # Add sampling parameters if sampling is enabled\n",
    "        if do_sample:\n",
    "            generation_kwargs[\"temperature\"] = temperature\n",
    "            generation_kwargs[\"top_p\"] = top_p\n",
    "        \n",
    "        # Start generation in a separate thread with torch.no_grad()               \n",
    "        def generate_with_no_grad():                                               \n",
    "          with torch.no_grad():                                                  \n",
    "              self.model.generate(**generation_kwargs)                           \n",
    "        \n",
    "        thread = Thread(target=generate_with_no_grad)                              \n",
    "        thread.start() \n",
    "        \n",
    "        # Collect generated text\n",
    "        generated_text = \"\"\n",
    "        for text_chunk in streamer:\n",
    "            generated_text += text_chunk\n",
    "            yield text_chunk\n",
    "        \n",
    "        # Wait for generation to complete\n",
    "        thread.join()\n",
    "\n",
    "        # Clean up tensors immediately\n",
    "        del inputs\n",
    "        \n",
    "        # Clear GPU cache if using CUDA\n",
    "        if self.device == \"cuda\" and torch.cuda.is_available():\n",
    "            torch.cuda.empty_cache()\n",
    "        \n",
    "        # Return final result\n",
    "        return TranscriptionResult(\n",
    "            text=generated_text.strip(),\n",
    "            confidence=None,\n",
    "            segments=None,\n",
    "            metadata={\n",
    "                \"model\": model_id,\n",
    "                \"language\": language or \"en\",\n",
    "                \"device\": self.device,\n",
    "                \"dtype\": str(self.dtype),\n",
    "                \"streaming\": True,\n",
    "            }\n",
    "        )\n",
    "        \n",
    "    finally:\n",
    "        # Clean up temporary file if created\n",
    "        if temp_file_created:\n",
    "            try:\n",
    "                Path(audio_path).unlink()\n",
    "            except Exception:\n",
    "                pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f956ac0",
   "metadata": {},
   "source": [
    "## Testing the Plugin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "521bac38",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Voxtral available: True\n",
      "Plugin name: voxtral_hf\n",
      "Plugin version: 1.0.0\n",
      "Supported formats: ['wav', 'mp3', 'flac', 'm4a', 'ogg', 'webm', 'mp4', 'avi', 'mov']\n",
      "Config class: VoxtralHFPluginConfig\n",
      "Supports streaming: True\n"
     ]
    }
   ],
   "source": [
    "# Test basic functionality\n",
    "plugin = VoxtralHFPlugin()\n",
    "\n",
    "# Check availability\n",
    "print(f\"Voxtral available: {plugin.is_available()}\")\n",
    "print(f\"Plugin name: {plugin.name}\")\n",
    "print(f\"Plugin version: {plugin.version}\")\n",
    "print(f\"Supported formats: {plugin.supported_formats}\")\n",
    "print(f\"Config class: {plugin.config_class.__name__}\")\n",
    "print(f\"Supports streaming: {plugin.supports_streaming()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76e26b64",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Available models:\n",
      "  - mistralai/Voxtral-Mini-3B-2507\n",
      "  - mistralai/Voxtral-Small-24B-2507\n"
     ]
    }
   ],
   "source": [
    "# Test configuration dataclass\n",
    "from dataclasses import fields\n",
    "\n",
    "print(\"Available models:\")\n",
    "model_field = next(f for f in fields(VoxtralHFPluginConfig) if f.name == \"model_id\")\n",
    "for model in model_field.metadata.get(SCHEMA_ENUM, []):\n",
    "    print(f\"  - {model}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f446f47",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Valid config: Valid=True\n",
      "Invalid model: Valid=False\n",
      "  Error: model_id: 'invalid_model' is not one of ['mistralai/Voxtral-Mini-3B-2507', 'mistralai/Voxtral-Small-\n",
      "Temperature out of range: Valid=False\n",
      "  Error: temperature: 2.5 is greater than maximum 2.0\n"
     ]
    }
   ],
   "source": [
    "# Test configuration validation\n",
    "test_configs = [\n",
    "    ({\"model_id\": \"mistralai/Voxtral-Mini-3B-2507\"}, \"Valid config\"),\n",
    "    ({\"model_id\": \"invalid_model\"}, \"Invalid model\"),\n",
    "    ({\"model_id\": \"mistralai/Voxtral-Mini-3B-2507\", \"temperature\": 2.5}, \"Temperature out of range\"),\n",
    "]\n",
    "\n",
    "for config, description in test_configs:\n",
    "    try:\n",
    "        test_cfg = dict_to_config(VoxtralHFPluginConfig, config, validate=True)\n",
    "        print(f\"{description}: Valid=True\")\n",
    "    except ValueError as e:\n",
    "        print(f\"{description}: Valid=False\")\n",
    "        print(f\"  Error: {str(e)[:100]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed1b6bd3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current config (dict): model_id=mistralai/Voxtral-Mini-3B-2507\n"
     ]
    }
   ],
   "source": [
    "# Test initialization and get_current_config (returns dict now)\n",
    "plugin.initialize({\"model_id\": \"mistralai/Voxtral-Mini-3B-2507\", \"device\": \"cpu\"})\n",
    "current_config = plugin.get_current_config()\n",
    "print(f\"Current config (dict): model_id={current_config['model_id']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ta6dcyjadx",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "JSON Schema for VoxtralHFPluginConfig:\n",
      "  Name: VoxtralHFPluginConfig\n",
      "  Properties count: 14\n",
      "  Model field enum: ['mistralai/Voxtral-Mini-3B-2507', 'mistralai/Voxtral-Small-24B-2507']\n",
      "\n",
      "Sample properties:\n",
      "{\n",
      "  \"model_id\": {\n",
      "    \"type\": \"string\",\n",
      "    \"title\": \"Model ID\",\n",
      "    \"description\": \"Voxtral model to use. Mini is faster, Small is more accurate.\",\n",
      "    \"enum\": [\n",
      "      \"mistralai/Voxtral-Mini-3B-2507\",\n",
      "      \"mistralai/Voxtral-Small-24B-2507\"\n",
      "    ],\n",
      "    \"default\": \"mistralai/Voxtral-Mini-3B-2507\"\n",
      "  },\n",
      "  \"device\": {\n",
      "    \"type\": \"string\",\n",
      "    \"title\": \"Device\",\n",
      "    \"description\": \"Device for inference (auto will use CUDA if available)\",\n",
      "    \"enum\": [\n",
      "      \"auto\",\n",
      "      \"cpu\",\n",
      "      \"cuda\"\n",
      "    ],\n",
      "    \"default\": \"auto\"\n",
      "  },\n",
      "  \"dtype\": {\n",
      "    \"type\": \"string\",\n",
      "    \"title\": \"Data Type\",\n",
      "    \"description\": \"Data type for model weights (auto will use bfloat16 on GPU, float32 on CPU)\",\n",
      "    \"enum\": [\n",
      "      \"auto\",\n",
      "      \"bfloat16\",\n",
      "      \"float16\",\n",
      "      \"float32\"\n",
      "    ],\n",
      "    \"default\": \"auto\"\n",
      "  }\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "#| eval: false\n",
    "# Test get_config_schema for UI generation\n",
    "import json\n",
    "\n",
    "schema = plugin.get_config_schema()\n",
    "print(\"JSON Schema for VoxtralHFPluginConfig:\")\n",
    "print(f\"  Name: {schema['name']}\")\n",
    "print(f\"  Properties count: {len(schema['properties'])}\")\n",
    "print(f\"  Model field enum: {schema['properties']['model_id'].get('enum', [])}\")\n",
    "print(f\"\\nSample properties:\")\n",
    "print(json.dumps({k: v for k, v in list(schema['properties'].items())[:3]}, indent=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84faec63",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "import nbdev; nbdev.nbdev_export()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
