{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9118cd37-278d-4e47-8250-1eba8b6c922e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import VoxtralForConditionalGeneration, AutoProcessor, TextStreamer, TextIteratorStreamer\n",
    "import torch\n",
    "from threading import Thread"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7895a11c-682b-4aa4-961a-5953802b305f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# device = \"cpu\"\n",
    "device = \"cuda\"\n",
    "repo_id = \"mistralai/Voxtral-Mini-3B-2507\"\n",
    "# repo_id = \"mistralai/Voxtral-Small-24B-2507\"\n",
    "max_new_tokens = 25000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "58ee3f5b-189c-4e52-8e6b-faa9af27c979",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "de7b7e21aaa94c9789349bb49428c15b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 1 files:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "feaf24b6f2b04194a9dc7c5c49c59201",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 1 files:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6edd84c4ba9b484d9ffdfa3edef1d8e6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "processor = AutoProcessor.from_pretrained(repo_id)\n",
    "model = VoxtralForConditionalGeneration.from_pretrained(repo_id, dtype=torch.bfloat16, device_map=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b0e7b0d5-06a7-4776-8daf-165ab4270842",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nbdev.config import get_config\n",
    "from pathlib import Path\n",
    "\n",
    "config = get_config()\n",
    "project_dir = config.config_path\n",
    "test_dir = project_dir/\"./test_files/\"\n",
    "audio_path = test_dir/\"short_test_audio.mp3\"\n",
    "# audio_path = test_dir/\"02 - 1. Laying Plans.mp3\"\n",
    "assert audio_path.exists()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5d054ff-e18e-4d84-8b70-de0432e0ec70",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "1180d92b-725c-4165-8e23-49eafbc859b3",
   "metadata": {},
   "source": [
    "## Standard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1b42aed5-9f63-47b0-8559-136dea07c7e1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"November the 10th, Wednesday, 9 p.m. I'm standing in a dark alley. After waiting several hours, the time has come. A woman with long dark hair approaches. I have to act and fast before she realises what has happened. I must find out.\""
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs = processor.apply_transcription_request(language=\"en\", \n",
    "                                               audio=str(audio_path), \n",
    "                                               model_id=repo_id, \n",
    "                                              )\n",
    "inputs = inputs.to(device, dtype=torch.bfloat16)\n",
    "\n",
    "with torch.no_grad():\n",
    "    chunk_outputs = model.generate(\n",
    "        **inputs,\n",
    "        max_new_tokens=max_new_tokens,\n",
    "        streamer=None,\n",
    "        do_sample=False\n",
    "    )\n",
    "torch.cuda.empty_cache()\n",
    "result = processor.batch_decode(chunk_outputs[:, inputs.input_ids.shape[1]:], skip_special_tokens=True)[0]\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad421344-6882-45a4-9761-29cb20997c16",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "2d15a0ff-4b17-4d35-a597-61818a083615",
   "metadata": {},
   "source": [
    "## Standard with Streamer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8a3f994d-ce24-4df0-ac17-6f5dfd4143b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "streamer = TextStreamer(processor.tokenizer, skip_prompt=True, skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bcfec62e-bc4a-438c-af5d-5db26d49bc98",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "November the 10th, Wednesday, 9 p.m. I'm standing in a dark alley. After waiting several hours, the time has come. A woman with long dark hair approaches. I have to act and fast before she realises what has happened. I must find out.\n"
     ]
    }
   ],
   "source": [
    "inputs = processor.apply_transcription_request(language=\"en\", \n",
    "                                               audio=str(audio_path), \n",
    "                                               model_id=repo_id, \n",
    "                                              )\n",
    "inputs = inputs.to(device, dtype=torch.bfloat16)\n",
    "\n",
    "with torch.no_grad():\n",
    "    chunk_outputs = model.generate(\n",
    "        **inputs,\n",
    "        max_new_tokens=max_new_tokens,\n",
    "        streamer=streamer,\n",
    "        do_sample=False\n",
    "    )\n",
    "torch.cuda.empty_cache()\n",
    "result = processor.batch_decode(chunk_outputs[:, inputs.input_ids.shape[1]:], skip_special_tokens=True)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25f73ce3-38fb-4949-a537-6c54a9756058",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "3be6b777-429b-4865-83d5-5a4c4de47ade",
   "metadata": {},
   "source": [
    "## Non-blocking with Streamer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4c030e91-62dd-4f58-a444-44046f9f3d60",
   "metadata": {},
   "outputs": [],
   "source": [
    "streamer = TextIteratorStreamer(processor.tokenizer, skip_prompt=True, skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ce331edb-203b-4516-a020-5a7245cb0761",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_text_stream(model, inputs, max_new_tokens, streamer, return_full=False):\n",
    "    generation_kwargs = dict(\n",
    "        **inputs,\n",
    "        max_new_tokens=max_new_tokens,\n",
    "        streamer=streamer,\n",
    "        do_sample=False\n",
    "    )\n",
    "    \n",
    "    thread = Thread(target=model.generate, kwargs=generation_kwargs)\n",
    "    thread.start()\n",
    "    \n",
    "    generated_text = \"\"\n",
    "    for new_text in streamer:\n",
    "        generated_text += new_text\n",
    "        yield new_text\n",
    "    \n",
    "    thread.join()\n",
    "    \n",
    "    # Optionally return the complete text at the end\n",
    "    if return_full:\n",
    "        yield {\"complete_text\": generated_text}\n",
    "    torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f896007f-9911-4894-b9d6-a53e0fbd342c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "November the 10th, Wednesday, 9 p.m. I'm standing in a dark alley. After waiting several hours, the time has come. A woman with long dark hair approaches. I have to act and fast before she realises what has happened. I must find out."
     ]
    }
   ],
   "source": [
    "# Usage example:\n",
    "for text_chunk in generate_text_stream(model, inputs, max_new_tokens, streamer):\n",
    "    print(text_chunk, end=\"\", flush=True)\n",
    "    # Or in a web framework like FastAPI/Flask, you could stream this directly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c491002-2613-4bf9-ae6a-45a44a5f95c6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fa199bc-27c6-4727-9fcb-4f4836454387",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "badb60bb-2795-41d9-9819-61607cc0325a",
   "metadata": {},
   "outputs": [],
   "source": [
    "del model\n",
    "torch.cuda.empty_cache()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
